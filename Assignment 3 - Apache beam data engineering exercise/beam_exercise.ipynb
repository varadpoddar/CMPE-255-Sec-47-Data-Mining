{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbf476f",
   "metadata": {},
   "source": [
    "# Apache Beam Data Engineering Exercise (Colab-ready)\n",
    "\n",
    "Demonstrates core Beam transforms: Map, Filter, ParDo, composite transforms, Partition, windowing, and simple pipeline I/O. Optional: Beam ML RunInference stub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809b5c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Ensure dependencies are available (Beam + sklearn)\n",
    "%pip install -q -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1161782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam version: 2.56.0\n"
     ]
    }
   ],
   "source": [
    "# Install Beam (commented by default; uncomment in a fresh Colab runtime)\n",
    "# !pip install apache-beam[interactive]\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import tempfile, os, json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Beam version:\", beam.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae7a61",
   "metadata": {},
   "source": [
    "## Sample data\n",
    "We'll work with small, in-memory records to keep things fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "554b08a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': 'alice', 'score': 5, 'ts': 0},\n",
       " {'user': 'bob', 'score': 7, 'ts': 10},\n",
       " {'user': 'carol', 'score': 2, 'ts': 20},\n",
       " {'user': 'dave', 'score': 9, 'ts': 80},\n",
       " {'user': 'eve', 'score': 4, 'ts': 95}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = [\n",
    "    {\"user\": \"alice\", \"score\": 5, \"ts\": 0},\n",
    "    {\"user\": \"bob\", \"score\": 7, \"ts\": 10},\n",
    "    {\"user\": \"carol\", \"score\": 2, \"ts\": 20},\n",
    "    {\"user\": \"dave\", \"score\": 9, \"ts\": 80},\n",
    "    {\"user\": \"eve\", \"score\": 4, \"ts\": 95},\n",
    "]\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f135b0e",
   "metadata": {},
   "source": [
    "## Composite transform\n",
    "Combines map + filter into a reusable `PTransform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb57e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanAndFilterScores(beam.PTransform):\n",
    "    def __init__(self, min_score: int = 5):\n",
    "        super().__init__()\n",
    "        self.min_score = min_score\n",
    "\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | \"NormalizeUser\" >> beam.Map(lambda r: {**r, \"user\": r[\"user\"].strip().lower()})\n",
    "            | \"FilterByScore\" >> beam.Filter(lambda r: r[\"score\"] >= self.min_score)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158de04",
   "metadata": {},
   "source": [
    "## Main pipeline: Map, Filter, ParDo, Partition, I/O\n",
    "- Apply composite transform\n",
    "- ParDo to tag pass/fail\n",
    "- Partition even/odd scores\n",
    "- Write to text\n",
    "- Collect results for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c91ab99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp output dir: /var/folders/86/nr8r5c99659ggl44tw4xs62h0000gn/T/tmphwvw4tg0\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'even': <PCollection[[6]: EvenToList/[6]: EvenToList/UnKey.None] at 0x12c78da90>,\n",
      " 'odd': <PCollection[[6]: OddToList/[6]: OddToList/UnKey.None] at 0x12c6822d0>,\n",
      " 'tagged': <PCollection[[6]: TaggedToList/[6]: TaggedToList/UnKey.None] at 0x12c3eea50>}\n",
      "Sample output files:\n",
      " - beam_output-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "tmp_dir = tempfile.mkdtemp()\n",
    "output_path = os.path.join(tmp_dir, \"beam_output\")\n",
    "print(\"Temp output dir:\", tmp_dir)\n",
    "\n",
    "class TagPassFail(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        tag = \"pass\" if element[\"score\"] >= 6 else \"fail\"\n",
    "        element[\"tag\"] = tag\n",
    "        yield element\n",
    "\n",
    "def split_even_odd(element, num_partitions):\n",
    "    return 0 if element[\"score\"] % 2 == 0 else 1\n",
    "\n",
    "with beam.Pipeline(options=PipelineOptions(flags=[])) as p:\n",
    "    pcoll = p | \"CreateRecords\" >> beam.Create(records)\n",
    "\n",
    "    cleaned = pcoll | \"CleanAndFilter\" >> CleanAndFilterScores(min_score=3)\n",
    "    tagged = cleaned | \"TagPassFail\" >> beam.ParDo(TagPassFail())\n",
    "\n",
    "    partitions = tagged | \"PartitionEvenOdd\" >> beam.Partition(split_even_odd, 2)\n",
    "    even_scores = partitions[0]\n",
    "    odd_scores = partitions[1]\n",
    "\n",
    "    # Simple write to text\n",
    "    _ = tagged | \"WriteAll\" >> beam.io.WriteToText(output_path)\n",
    "\n",
    "    # Gather side results for display\n",
    "    tagged_res = tagged | \"TaggedToList\" >> beam.combiners.ToList().without_defaults()\n",
    "    even_res = even_scores | \"EvenToList\" >> beam.combiners.ToList().without_defaults()\n",
    "    odd_res = odd_scores | \"OddToList\" >> beam.combiners.ToList().without_defaults()\n",
    "\n",
    "    results = {\n",
    "        \"tagged\": tagged_res,\n",
    "        \"even\": even_res,\n",
    "        \"odd\": odd_res,\n",
    "    }\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(results)\n",
    "\n",
    "print(\"Sample output files:\")\n",
    "for f in os.listdir(tmp_dir):\n",
    "    print(\" -\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79756e22",
   "metadata": {},
   "source": [
    "## Windowing example (FixedWindows + aggregation)\n",
    "We assign timestamps to elements, window into 60s buckets, and count records per window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4106fcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windowed counts (per 60s window): PCollection[[7]: WindowedToList/[7]: WindowedToList/UnKey.None]\n"
     ]
    }
   ],
   "source": [
    "from apache_beam import window\n",
    "\n",
    "timestamped = [beam.window.TimestampedValue(r, r[\"ts\"]) for r in records]\n",
    "\n",
    "with beam.Pipeline(options=PipelineOptions(flags=[])) as p:\n",
    "    counts = (\n",
    "        p\n",
    "        | \"CreateTimestamped\" >> beam.Create(timestamped)\n",
    "        | \"WindowInto60s\" >> beam.WindowInto(window.FixedWindows(60))\n",
    "        | \"MapToOnes\" >> beam.Map(lambda r: (\"window\", 1))\n",
    "        | \"CountPerWindow\" >> beam.CombinePerKey(sum)\n",
    "    )\n",
    "\n",
    "    windowed = counts | \"WindowedToList\" >> beam.combiners.ToList().without_defaults()\n",
    "\n",
    "print(\"Windowed counts (per 60s window):\", windowed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b4d0c",
   "metadata": {},
   "source": [
    "## Inspect written output\n",
    "Read a sample of the written text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7459ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text outputs: ['/var/folders/86/nr8r5c99659ggl44tw4xs62h0000gn/T/tmphwvw4tg0/beam_output-00000-of-00001']\n",
      "\n",
      "Sample file contents:\n",
      " {'user': 'alice', 'score': 5, 'ts': 0, 'tag': 'fail'}\n",
      "{'user': 'bob', 'score': 7, 'ts': 10, 'tag': 'pass'}\n",
      "{'user': 'dave', 'score': 9, 'ts': 80, 'tag': 'pass'}\n",
      "{'user': 'eve', 'score': 4, 'ts': 95, 'tag': 'fail'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "txt_files = sorted(glob(output_path + \"*\") )\n",
    "print(\"Text outputs:\", txt_files)\n",
    "\n",
    "if txt_files:\n",
    "    with open(txt_files[0], \"r\") as f:\n",
    "        print(\"\\nSample file contents:\\n\", f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15480804",
   "metadata": {},
   "source": [
    "## Beam ML RunInference (sklearn)\n",
    "Train a tiny LogisticRegression model, save with joblib, and run Beam RunInference on two example points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc4e3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.0\n",
      "Prediction: 1.0\n",
      "Prediction: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "model = LogisticRegression().fit([[0, 0], [1, 1]], [0, 1])\n",
    "# Save as joblib and instruct handler accordingly\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".joblib\") as f:\n",
    "    joblib.dump(model, f.name)\n",
    "    model_path = f.name\n",
    "\n",
    "from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpy, ModelFileType\n",
    "from apache_beam.ml.inference.base import ModelHandler, RunInference\n",
    "\n",
    "handler: ModelHandler = SklearnModelHandlerNumpy(\n",
    "    model_uri=model_path,\n",
    "    model_file_type=ModelFileType.JOBLIB,\n",
    ")\n",
    "examples = [np.array([0, 0]), np.array([1, 1]), np.array([2, 2])]\n",
    "with beam.Pipeline(options=PipelineOptions(flags=[])) as p:\n",
    "    _ = (\n",
    "        p\n",
    "        | beam.Create(examples)\n",
    "        | RunInference(handler)\n",
    "        | beam.Map(lambda res: float(np.atleast_1d(res.inference)[0]))\n",
    "        | beam.Map(lambda pred: print(f\"Prediction: {pred}\") or pred)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857da18",
   "metadata": {},
   "source": [
    "## Notes for your screencast\n",
    "- Show Map/Filter/ParDo/Partition outputs.\n",
    "- Explain how the composite transform bundles steps.\n",
    "- Highlight windowed counts and where files are written.\n",
    "- (Optional) RunInference demo if you enable the cell above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
