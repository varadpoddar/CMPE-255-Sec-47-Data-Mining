{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mental Health Survey CRISP-DM Notebook\n",
    "\n",
    "Use this notebook on Colab or locally to run the full pipeline: data download, EDA, preprocessing, clustering, outlier detection, and happiness regression.\n",
    "\n",
    "**Checklist before running:**\n",
    "- Upload the Kaggle dataset CSV to a path you set in `DATA_PATH` or configure the Kaggle API cell.\n",
    "- Ensure runtime has Python 3.10+.\n",
    "- If running locally, `pip install -r requirements.txt` first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Optional: install dependencies on Colab (safe to skip if already installed)\n",
    "!pip -q install -r \"requirements.txt\" || echo \"Install locally as needed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor, IsolationForest, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure data location\n",
    "- Set `DATA_PATH` to your Kaggle CSV.\n",
    "- Optional: uncomment the Kaggle API cell if you want to download programmatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paths"
   },
   "outputs": [],
   "source": [
    "# Path to the mental health CSV (update this for your environment)\n",
    "DATA_PATH = pathlib.Path(\"./Mental_Health_and_Social_Media_Balance_Dataset.csv\")\n",
    "TARGET_COL = \"Happiness_Index(1-10)\"  # target present in the provided dataset\n",
    "\n",
    "# Example: configure Kaggle API if running on Colab (requires kaggle.json upload)\n",
    "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# api = KaggleApi(); api.authenticate()\n",
    "# api.dataset_download_files(\"USERNAME/DATASET-NAME\", path=\"./data\", unzip=True)\n",
    "# DATA_PATH = pathlib.Path(\"./data/your_file.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "assert DATA_PATH.exists(), f\"CSV not found at {DATA_PATH}. Please update DATA_PATH.\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "info"
   },
   "outputs": [],
   "source": [
    "# Quick info\n",
    "display(df.dtypes)\n",
    "display(df.describe(include=\"all\"))\n",
    "\n",
    "def missingness_table(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    miss_pct = frame.isna().mean().sort_values(ascending=False) * 100\n",
    "    return miss_pct.to_frame(name=\"missing_pct\").query(\"missing_pct > 0\")\n",
    "\n",
    "missingness_table(df).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column typing\n",
    "- Update `CAT_COLS` and `NUM_COLS` if you want to override the automatic detection.\n",
    "- The target column is removed from predictors automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "columns"
   },
   "outputs": [],
   "source": [
    "# Manually set these if auto-detection is not correct for your dataset\n",
    "CAT_COLS: List[str] = []\n",
    "NUM_COLS: List[str] = []\n",
    "\n",
    "def auto_columns(frame: pd.DataFrame, target: str) -> Dict[str, List[str]]:\n",
    "    cat = frame.select_dtypes(include=[\"object\", \"category\", \"bool\"])\n",
    "    num = frame.select_dtypes(include=[\"number\"]).copy()\n",
    "    cat_cols = [c for c in cat.columns if c != target]\n",
    "    num_cols = [c for c in num.columns if c != target]\n",
    "    return {\"cat\": cat_cols, \"num\": num_cols}\n",
    "\n",
    "if not CAT_COLS and not NUM_COLS:\n",
    "    columns = auto_columns(df, TARGET_COL)\n",
    "    CAT_COLS, NUM_COLS = columns[\"cat\"], columns[\"num\"]\n",
    "\n",
    "FEATURE_COLS = CAT_COLS + NUM_COLS\n",
    "print(\"Categorical: \", CAT_COLS)\n",
    "print(\"Numeric: \", NUM_COLS)\n",
    "assert TARGET_COL in df.columns, \"Target column not found. Update TARGET_COL.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: distributions and correlations\n",
    "- Run a small set of plots to understand distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "univariate"
   },
   "outputs": [],
   "source": [
    "# Numeric distributions\n",
    "num_sample = df[NUM_COLS].sample(min(len(df), 5000), random_state=RANDOM_STATE)\n",
    "num_sample.hist(figsize=(14, 10), bins=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Categorical distributions (top 15 categories)\n",
    "for col in CAT_COLS[:8]:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    df[col].value_counts(dropna=False).head(15).plot(kind=\"bar\")\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "correlations"
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric features\n",
    "if NUM_COLS:\n",
    "    corr = df[NUM_COLS + [TARGET_COL]].corr(numeric_only=True)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=False, cmap=\"coolwarm\", center=0)\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/validation/test split and preprocessing\n",
    "- Uses 70/15/15 split with simple imputers, one-hot encoding, and scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess"
   },
   "outputs": [],
   "source": [
    "X = df[FEATURE_COLS].copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=RANDOM_STATE\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", categorical_transformer, CAT_COLS),\n",
    "        (\"numeric\", numeric_transformer, NUM_COLS),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and validation\n",
    "- Baseline and lightweight models compared using MAE, RMSE, R^2 on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "models"
   },
   "outputs": [],
   "source": [
    "def regression_metrics(y_true, y_pred) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"rmse\": mean_squared_error(y_true, y_pred) ** 0.5,\n",
    "        \"r2\": r2_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "models = {\n",
    "    \"dummy_mean\": DummyRegressor(strategy=\"mean\"),\n",
    "    \"linear\": LinearRegression(),\n",
    "    \"ridge\": Ridge(alpha=1.0, random_state=RANDOM_STATE),\n",
    "    \"lasso\": Lasso(alpha=0.001, random_state=RANDOM_STATE, max_iter=5000),\n",
    "    \"rf\": RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=8, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    ),\n",
    "    \"gbr\": GradientBoostingRegressor(random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_val)\n",
    "    metrics = regression_metrics(y_val, preds)\n",
    "    row = {\"model\": name, **metrics}\n",
    "    results.append(row)\n",
    "    trained_models[name] = pipe\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"mae\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_eval"
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model on the held-out test set\n",
    "best_model_name = results_df.iloc[0][\"model\"]\n",
    "best_model = trained_models[best_model_name]\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_metrics = regression_metrics(y_test, test_preds)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(json.dumps(test_metrics, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "importance"
   },
   "outputs": [],
   "source": [
    "# Permutation importance on validation data for the best model\n",
    "perm = permutation_importance(\n",
    "    best_model, X_val, y_val, n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Align names to original feature columns (permutation_importance permutes original X)\n",
    "feature_names = FEATURE_COLS\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_mean\": perm.importances_mean,\n",
    "        \"importance_std\": perm.importances_std,\n",
    "    }\n",
    ").sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "importance_df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (k-means) for respondent segments\n",
    "- Uses scaled numeric features; categorical columns are one-hot encoded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clustering"
   },
   "outputs": [],
   "source": [
    "# Build a numeric-only view for clustering\n",
    "numeric_view = df[NUM_COLS].copy()\n",
    "numeric_view = numeric_view.fillna(numeric_view.median())\n",
    "scaled_numeric = StandardScaler().fit_transform(numeric_view)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)\n",
    "clusters = kmeans.fit_predict(scaled_numeric)\n",
    "df_clusters = df.copy()\n",
    "df_clusters[\"cluster\"] = clusters\n",
    "\n",
    "cluster_summary = df_clusters.groupby(\"cluster\").agg({\n",
    "    TARGET_COL: [\"mean\", \"median\"],\n",
    "})\n",
    "cluster_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection\n",
    "- Isolation Forest on scaled numeric features; adjusts contamination to ~2% by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "outliers"
   },
   "outputs": [],
   "source": [
    "iso = IsolationForest(contamination=0.02, random_state=RANDOM_STATE)\n",
    "outlier_flags = iso.fit_predict(scaled_numeric)\n",
    "df_outliers = df.copy()\n",
    "df_outliers[\"outlier\"] = (outlier_flags == -1)\n",
    "print(df_outliers[\"outlier\"].value_counts())\n",
    "df_outliers[df_outliers[\"outlier\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Calibrate models with hyperparameter search (grid or randomized) if compute allows.\n",
    "- Add fairness checks for demographic subgroups where applicable.\n",
    "- Refresh metrics when new survey vintages arrive and monitor for label drift.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
