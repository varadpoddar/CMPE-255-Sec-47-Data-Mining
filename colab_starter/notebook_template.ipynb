{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Notebook Template (Colab-ready)\n",
    "\n",
    "Use this template to run EDA, preprocessing, and lightweight modeling on a tabular dataset. Set the dataset path/target below and run top-to-bottom. Keep compute light; subsample if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies (uncomment on Colab if needed)\n",
    "# !pip -q install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure data and target\n",
    "- Update `DATA_PATH` to your CSV.\n",
    "- Set `TARGET_COL` to the column you want to predict (numeric regression assumed; swap models/metrics if classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = pathlib.Path(\"./your_dataset.csv\")\n",
    "TARGET_COL = \"target_column\"\n",
    "ID_COLS: List[str] = []  # e.g., [\"id\"] to drop from features\n",
    "\n",
    "assert DATA_PATH.exists(), f\"CSV not found at {DATA_PATH}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "display(df.head())\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "print(\"Missing values per column:\\n\", df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column typing (auto, override if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = [c for c in cat_cols if c not in ID_COLS + [TARGET_COL]]\n",
    "num_cols = [c for c in num_cols if c not in ID_COLS + [TARGET_COL]]\n",
    "feature_cols = cat_cols + num_cols\n",
    "print(\"Categorical:\", cat_cols)\n",
    "print(\"Numeric:\", num_cols)\n",
    "assert TARGET_COL in df.columns, \"Target column not found.\"\n",
    "assert feature_cols, \"No feature columns detected; adjust column typing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric distributions\n",
    "if num_cols:\n",
    "    df[num_cols].hist(figsize=(12, 8), bins=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Categorical counts\n",
    "for col in cat_cols:\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.countplot(x=col, data=df, order=df[col].value_counts().index)\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations and target relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_cols:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df[num_cols + [TARGET_COL]].corr(), annot=False, cmap=\"coolwarm\", center=0)\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.scatterplot(x=df[col], y=df[TARGET_COL], alpha=0.5)\n",
    "    sns.regplot(x=df[col], y=df[TARGET_COL], scatter=False, color=\"red\")\n",
    "    plt.title(f\"{TARGET_COL} vs {col}\")\n",
    "    plt.show()\n",
    "\n",
    "for col in cat_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=df[col], y=df[TARGET_COL])\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.title(f\"{TARGET_COL} by {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols].copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "\n    # train/val/test: 70/15/15\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=RANDOM_STATE)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=RANDOM_STATE)\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    ")\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", categorical_transformer, cat_cols),\n",
    "        (\"numeric\", numeric_transformer, num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def regression_metrics(y_true, y_pred) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"rmse\": mean_squared_error(y_true, y_pred) ** 0.5,\n",
    "        \"r2\": r2_score(y_true, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models (regression baseline and light learners)\n",
    "- Swap these for classifiers if your target is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"dummy_mean\": DummyRegressor(strategy=\"mean\"),\n",
    "    \"linear\": LinearRegression(),\n",
    "    \"ridge\": Ridge(alpha=1.0, random_state=RANDOM_STATE),\n",
    "    \"lasso\": Lasso(alpha=0.001, random_state=RANDOM_STATE, max_iter=5000),\n",
    "    \"rf\": RandomForestRegressor(n_estimators=200, max_depth=8, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"gbr\": GradientBoostingRegressor(random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_val)\n",
    "    metrics = regression_metrics(y_val, preds)\n",
    "    results.append({\"model\": name, **metrics})\n",
    "    trained_models[name] = pipe\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"mae\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best on test\n",
    "best_model_name = results_df.iloc[0][\"model\"]\n",
    "best_model = trained_models[best_model_name]\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_metrics = regression_metrics(y_test, test_preds)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(json.dumps(test_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance (feature signals)\n",
    "- Uses original feature columns (pre-encoding) to keep interpretation simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = permutation_importance(\n",
    "    best_model, X_val, y_val, n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature_cols,\n",
    "        \"importance_mean\": perm.importances_mean,\n",
    "        \"importance_std\": perm.importances_std,\n",
    "    }\n",
    ").sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "importance_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Add classification models/metrics if the target is categorical.\n",
    "- Tune hyperparameters (grid/random search) if compute allows.\n",
    "- Save figures (e.g., to `figures/`) and update your README with metrics/insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
