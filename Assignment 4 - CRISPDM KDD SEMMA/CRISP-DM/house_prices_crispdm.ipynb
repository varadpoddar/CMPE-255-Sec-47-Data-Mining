{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c1d37e",
   "metadata": {},
   "source": [
    "# Assignment 4 — CRISP-DM Case Study (House Prices)\n",
    "\n",
    "Dataset: [`michaelmallari/house-prices-advanced-regression-techniques`](https://huggingface.co/datasets/michaelmallari/house-prices-advanced-regression-techniques)\n",
    "\n",
    "Goals:\n",
    "- Practice end-to-end CRISP-DM on a tabular regression task (predict `SalePrice`).\n",
    "- Compare baselines vs. tree-based models; report RMSE/MAE/R².\n",
    "- Save figures for documentation and support a short video walkthrough.\n",
    "\n",
    "Environment: Python 3.11 (`.venv`), scikit-learn, pandas, seaborn, datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "FIG_DIR = Path(\"figures\")\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RNG = 42\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "ds_train = load_dataset(\"michaelmallari/house-prices-advanced-regression-techniques\", split=\"train\")\n",
    "df = ds_train.to_pandas()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a908267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data checks\n",
    "target = \"SalePrice\"\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "missing_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(y, kde=True, bins=40)\n",
    "plt.title(\"SalePrice Distribution\")\n",
    "plt.xlabel(\"SalePrice\")\n",
    "plt.tight_layout()\n",
    "target_fig = FIG_DIR / \"target_distribution.png\"\n",
    "plt.savefig(target_fig)\n",
    "target_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ecdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for top numeric features w.r.t target\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "corr = df[numeric_cols.tolist() + [target]].corr()[target].drop(target)\n",
    "top_corr = corr.abs().sort_values(ascending=False).head(12).index\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df[list(top_corr) + [target]].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation (Top numeric vs SalePrice)\")\n",
    "plt.tight_layout()\n",
    "corr_fig = FIG_DIR / \"correlation_top_numeric.png\"\n",
    "plt.savefig(corr_fig)\n",
    "corr_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: split, then ColumnTransformer with imputers and one-hot\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RNG)\n",
    "\n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_cols = [c for c in X_train.columns if c not in categorical_cols]\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, name: str):\n",
    "    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2,\n",
    "        \"pipeline\": pipe,\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append(evaluate_model(DummyRegressor(strategy=\"median\"), \"Dummy (median)\"))\n",
    "results.append(evaluate_model(RandomForestRegressor(n_estimators=200, random_state=RNG, n_jobs=-1), \"RandomForest\"))\n",
    "results.append(evaluate_model(GradientBoostingRegressor(random_state=RNG), \"GradientBoosting\"))\n",
    "\n",
    "pd.DataFrame([{k: v for k, v in r.items() if k != \"pipeline\"} for r in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b62a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best model by RMSE\n",
    "best = sorted(results, key=lambda d: d[\"rmse\"])[0]\n",
    "best_name = best[\"name\"]\n",
    "best_pipe = best[\"pipeline\"]\n",
    "best_metrics = {k: v for k, v in best.items() if k in [\"rmse\", \"mae\", \"r2\"]}\n",
    "best_name, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b9430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance on raw feature set (uses original column names)\n",
    "X_sample = X_test.sample(n=min(200, len(X_test)), random_state=RNG)\n",
    "y_sample = y_test.loc[X_sample.index]\n",
    "perm = permutation_importance(best_pipe, X_sample, y_sample, n_repeats=8, random_state=RNG)\n",
    "importances = pd.Series(perm.importances_mean, index=X_sample.columns)\n",
    "top_imp = importances.sort_values(ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x=top_imp.values, y=top_imp.index)\n",
    "plt.title(f\"Top permutation importances ({best_name})\")\n",
    "plt.xlabel(\"Importance (mean decrease in score)\")\n",
    "plt.tight_layout()\n",
    "imp_fig = FIG_DIR / \"permutation_importance.png\"\n",
    "plt.savefig(imp_fig)\n",
    "imp_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f74cc",
   "metadata": {},
   "source": [
    "## Next steps / deployment considerations\n",
    "- Calibrate hyperparameters (e.g., tune RF depth/leaves, GB learning rate) via cross-validation.\n",
    "- Log model + preprocessing using `joblib` or `skops`, version datasets, and emit a prediction service (FastAPI) if deploying.\n",
    "- Add bias/fairness checks for neighborhood-related features; consider SHAP for local explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a27663",
   "metadata": {},
   "source": [
    "## Extended analysis: log target, tuning, residuals, segments, SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "import shap\n",
    "\n",
    "# Log-transform target experiment on the best model type\n",
    "log_model = GradientBoostingRegressor(random_state=RNG)\n",
    "log_pipe = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"tt\", TransformedTargetRegressor(\n",
    "        regressor=log_model,\n",
    "        func=lambda y: np.log1p(y),\n",
    "        inverse_func=lambda y: np.expm1(y)\n",
    "    ))\n",
    "])\n",
    "log_pipe.fit(X_train, y_train)\n",
    "log_preds = log_pipe.predict(X_test)\n",
    "log_rmse = mean_squared_error(y_test, log_preds, squared=False)\n",
    "log_mae = mean_absolute_error(y_test, log_preds)\n",
    "log_r2 = r2_score(y_test, log_preds)\n",
    "{\"rmse\": log_rmse, \"mae\": log_mae, \"r2\": log_r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Re-select best model including log-target variant\n",
    "candidates = results + [{\n",
    "    \"name\": \"GradientBoosting (log target)\",\n",
    "    \"rmse\": log_rmse,\n",
    "    \"mae\": log_mae,\n",
    "    \"r2\": log_r2,\n",
    "    \"pipeline\": log_pipe,\n",
    "}]\n",
    "best = sorted(candidates, key=lambda d: d[\"rmse\"])[0]\n",
    "best_name = best[\"name\"]\n",
    "best_pipe = best[\"pipeline\"]\n",
    "best_metrics = {k: v for k, v in best.items() if k in [\"rmse\", \"mae\", \"r2\"]}\n",
    "best_name, best_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick hyperparameter search for GradientBoosting (log-target) with 5 iterations\n",
    "param_dist = {\n",
    "    \"tt__regressor__n_estimators\": [200, 300, 400],\n",
    "    \"tt__regressor__learning_rate\": [0.05, 0.075, 0.1],\n",
    "    \"tt__regressor__max_depth\": [2, 3, 4],\n",
    "}\n",
    "base = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"tt\", TransformedTargetRegressor(\n",
    "        regressor=GradientBoostingRegressor(random_state=RNG),\n",
    "        func=lambda y: np.log1p(y),\n",
    "        inverse_func=lambda y: np.expm1(y)\n",
    "    )),\n",
    "])\n",
    "search = RandomizedSearchCV(\n",
    "    base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    random_state=RNG,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "search.fit(X, y)\n",
    "tuned_rmse = -search.best_score_\n",
    "{\"cv_rmse\": tuned_rmse, \"best_params\": search.best_params_}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Residual diagnostics using the selected best model\n",
    "preds = best_pipe.predict(X_test)\n",
    "residuals = y_test - preds\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=preds, y=residuals, alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual (actual - pred)')\n",
    "plt.title(f'Residuals vs Predicted ({best_name})')\n",
    "plt.tight_layout()\n",
    "resid_fig = FIG_DIR / 'residuals_vs_pred.png'\n",
    "plt.savefig(resid_fig)\n",
    "resid_fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56baabf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(residuals, bins=40, kde=True)\n",
    "plt.title('Residual distribution')\n",
    "plt.tight_layout()\n",
    "resid_hist_fig = FIG_DIR / 'residual_hist.png'\n",
    "plt.savefig(resid_hist_fig)\n",
    "resid_hist_fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726dae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Segment performance by Neighborhood (top 10 by count)\n",
    "seg = (\n",
    "    pd.DataFrame({\"Neighborhood\": X_test[\"Neighborhood\"], \"y\": y_test, \"pred\": preds})\n",
    "    .assign(abs_err=lambda d: (d[\"y\"] - d[\"pred\"]).abs())\n",
    ")\n",
    "seg_summary = (\n",
    "    seg.groupby(\"Neighborhood\")\n",
    "      .agg(count=(\"y\", \"size\"), mae=(\"abs_err\", \"mean\"))\n",
    "      .sort_values(\"count\", ascending=False)\n",
    "      .head(10)\n",
    ")\n",
    "seg_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff812ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3-fold CV on the selected best pipeline\n",
    "cv_scores = cross_val_score(best_pipe, X, y, cv=3, scoring='neg_root_mean_squared_error')\n",
    "{\"cv_rmse_mean\": (-cv_scores).mean(), \"cv_rmse_std\": (-cv_scores).std()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254675e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SHAP summary (sampled for speed)\n",
    "feature_names = best_pipe.named_steps[\"prep\"].get_feature_names_out()\n",
    "X_shap = X_test.sample(n=min(120, len(X_test)), random_state=RNG)\n",
    "X_proc = best_pipe.named_steps[\"prep\"].transform(X_shap)\n",
    "if hasattr(X_proc, \"toarray\"):\n",
    "    X_proc = X_proc.toarray()\n",
    "\n",
    "# Extract underlying regressor (handles log-target wrapper)\n",
    "reg = None\n",
    "if \"tt\" in best_pipe.named_steps:\n",
    "    reg = best_pipe.named_steps[\"tt\"].regressor_\n",
    "elif \"model\" in best_pipe.named_steps:\n",
    "    reg = best_pipe.named_steps[\"model\"]\n",
    "\n",
    "explainer = shap.Explainer(reg, X_proc, feature_names=feature_names)\n",
    "shap_values = explainer(X_proc)\n",
    "shap.plots.beeswarm(shap_values, max_display=15, show=False)\n",
    "plt.tight_layout()\n",
    "shap_fig = FIG_DIR / \"shap_beeswarm.png\"\n",
    "plt.savefig(shap_fig, bbox_inches=\"tight\")\n",
    "shap_fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Broader hyperparameter search for log-target GradientBoosting (more iterations)\n",
    "param_dist = {\n",
    "    \"tt__regressor__n_estimators\": [200, 300, 400, 500],\n",
    "    \"tt__regressor__learning_rate\": [0.05, 0.075, 0.1, 0.125],\n",
    "    \"tt__regressor__max_depth\": [2, 3, 4],\n",
    "}\n",
    "base = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"tt\", TransformedTargetRegressor(\n",
    "        regressor=GradientBoostingRegressor(random_state=RNG),\n",
    "        func=lambda y: np.log1p(y),\n",
    "        inverse_func=lambda y: np.expm1(y)\n",
    "    )),\n",
    "])\n",
    "search = RandomizedSearchCV(\n",
    "    base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=12,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    random_state=RNG,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "search.fit(X, y)\n",
    "best_tuned_rmse = -search.best_score_\n",
    "best_tuned_params = search.best_params_\n",
    "{\"cv_rmse\": best_tuned_rmse, \"best_params\": best_tuned_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d197c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "pdp_features = [\"OverallQual\", \"GrLivArea\", \"GarageCars\"]\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12,4))\n",
    "PartialDependenceDisplay.from_estimator(best_pipe, X_sample, features=pdp_features, ax=ax)\n",
    "plt.tight_layout()\n",
    "pdp_fig = FIG_DIR / \"pdp_top_features.png\"\n",
    "plt.savefig(pdp_fig, bbox_inches=\"tight\")\n",
    "pdp_fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group MAE by Neighborhood (top 8) and HouseStyle (all)\n",
    "groups = []\n",
    "for col, topn in [(\"Neighborhood\", 8), (\"HouseStyle\", None)]:\n",
    "    d = (\n",
    "        pd.DataFrame({col: X_test[col], \"y\": y_test, \"pred\": preds})\n",
    "        .assign(abs_err=lambda t: (t[\"y\"] - t[\"pred\"]).abs())\n",
    "        .groupby(col)\n",
    "        .agg(count=(\"y\", \"size\"), mae=(\"abs_err\", \"mean\"))\n",
    "        .sort_values(\"mae\", ascending=False)\n",
    "    )\n",
    "    if topn:\n",
    "        d = d.head(topn)\n",
    "    groups.append((col, d))\n",
    "\n",
    "fig, axes = plt.subplots(1, len(groups), figsize=(12,4))\n",
    "for ax, (col, d) in zip(axes, groups):\n",
    "    sns.barplot(x=d.mae, y=d.index, ax=ax)\n",
    "    ax.set_title(f\"MAE by {col}\")\n",
    "    ax.set_xlabel(\"MAE\")\n",
    "plt.tight_layout()\n",
    "group_fig = FIG_DIR / \"group_mae.png\"\n",
    "plt.savefig(group_fig, bbox_inches=\"tight\")\n",
    "group_fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba3306",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "- Log-target GradientBoosting remains best; broader tuning (12 draws) suggests slightly improved CV RMSE with n_estimators up to 500.\n",
    "- Key drivers stay consistent across permutation, SHAP, and PDP: overall quality, living area, garage, basement.\n",
    "- Residuals concentrate near zero but widen on high-price homes; group MAE highlights neighborhoods/house styles where errors spike.\n",
    "- Deployment should package preprocessing + model, include monitoring on slice metrics, and revisit feature engineering for high-error groups.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}